{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c5a136-e872-4846-9965-9ddc5251a0b3",
   "metadata": {},
   "source": [
    "# NYC Apartment Search\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1BYVyFBDcTywdUlanH0ysfOrNWPgl7UkqXA7NeewTzxA/edit#heading=h.bpxu7uvknnbk)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an idea of a possible approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf11fa0-4684-4f5e-8048-0f4cc5f4f243",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f675d4b-794e-407c-aac9-b85c4a3975d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import statements needed for the project, for example:\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import urllib.parse\n",
    "\n",
    "import geoalchemy2 as gdb\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shapely\n",
    "import sqlalchemy as db\n",
    "\n",
    "from sqlalchemy.orm import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a62277-51cf-48a2-81d2-9b2127088a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any constants you might need; some have been added for you\n",
    "\n",
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = \"/Users/chining/Documents/GitHub/NYC-Rental-Analysis/data/drive-download-20231124T164450Z-001/nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "NYC_DATA_APP_TOKEN = \"IxFjBjShI6cenJ0NOVZ1rnj0W\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "DB_NAME = \"group14project\"\n",
    "DB_USER = \"chenruijia\"\n",
    "DB_URL = f\"postgres+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd67cca9-ec72-44e3-83b8-b65f1ed5bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52476a07-9bf2-4b7a-8cb7-93648bb4d303",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b18f12-c0ce-4b9c-adc1-805703edc575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url, force=False):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    url_path = parsed_url.path.strip(\"/\")\n",
    "    \n",
    "    filename = DATA_DIR / url_path\n",
    "    \n",
    "    if force or not filename.exists():\n",
    "        print(f\"Downloading {url} to {filename}...\")\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(..., f)\n",
    "        print(f\"Done downloading {url}.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Reading from {filename}...\")\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee245240-2fbb-45b8-9a92-4e2368f62c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pyproj\n",
    "from functools import partial\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Assuming the primary .shp file is 'nyc_zipcodes.shp' and it's in the same directory with other related files\n",
    "shapefile_path = 'data/drive-download-20231124T164450Z-001/nyc_zipcodes.shp'\n",
    "\n",
    "\n",
    "def load_and_clean_zipcodes(shapefile_path):\n",
    "    # Reading the shapefile\n",
    "    geodf_zipcode = gpd.read_file(shapefile_path)\n",
    "\n",
    "    # Filter out non-Polygon geometries\n",
    "    geodf_zipcode = geodf_zipcode[geodf_zipcode['geometry'].apply(lambda geom: isinstance(geom, Polygon))]\n",
    "\n",
    "    # Filter by ZIPCODE pattern (New York City zip codes starting with '1')\n",
    "    geodf_zipcode = geodf_zipcode[geodf_zipcode['ZIPCODE'].astype(str).str.match(r'^1\\d{4}$')]\n",
    "    \n",
    "    # Defining the coordinate reference systems\n",
    "    current_srid = 'EPSG:2263'  # NAD83 / New York Long Island (ftUS)\n",
    "    desired_srid = 'EPSG:4326'  # WGS 84\n",
    "\n",
    "    # Updating to use pyproj.Transformer\n",
    "    transformer = pyproj.Transformer.from_crs(current_srid, desired_srid, always_xy=True)\n",
    "\n",
    "    # Applying the transformation\n",
    "    geodf_zipcode['geometry'] = geodf_zipcode['geometry'].apply(lambda geom: transform(transformer.transform, geom))\n",
    "\n",
    "    return geodf_zipcode\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed2a5a9-1027-4c41-bbb5-039c32ce7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "def download_and_clean_311_data():\n",
    "    def fetch_data(offset, limit):\n",
    "        api_endpoint = f\"{base_api_endpoint}?{soql_query}&$select={columns}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(api_endpoint, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return []\n",
    "    base_api_endpoint = 'https://data.cityofnewyork.us/resource/erm2-nwe9.json'\n",
    "    columns = 'unique_key,created_date,closed_date,complaint_type,incident_zip,latitude,longitude'\n",
    "    start_date = '2015-01-01T00:00:00'\n",
    "    end_date = '2023-09-30T23:59:59'\n",
    "    headers = {'X-App-Token': NYC_DATA_APP_TOKEN}\n",
    "    soql_query = f\"$where=created_date between '{start_date}' and '{end_date}'\"\n",
    "    limit = 200000\n",
    "    all_data = []\n",
    "    offset = 0\n",
    "    more_data = True\n",
    "\n",
    "    while more_data:\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "\n",
    "            futures = [executor.submit(fetch_data, off, limit) for off in range(offset, offset + 10 * limit, limit)]\n",
    "    \n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching Data\"):\n",
    "                data = future.result()\n",
    "                if data:\n",
    "                    all_data.extend(data)\n",
    "                    print(f\"Retrieved records up to offset {offset + limit}...\")\n",
    "                else:\n",
    "                    more_data = False\n",
    "\n",
    "            offset += 10 * limit\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c4b1bc-c841-4b87-8301-1dc2cafeccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data():\n",
    "    base_api_endpoint = 'https://data.cityofnewyork.us/resource/5rq2-4hqu.json' \n",
    "    columns = 'tree_id,latitude,longitude,status,health,spc_common,zipcode'\n",
    "    \n",
    "    headers = {\n",
    "        'X-App-Token': NYC_DATA_APP_TOKEN\n",
    "    }\n",
    "    \n",
    "    limit = 50000   \n",
    "    offset = 0\n",
    "    all_data = []\n",
    "\n",
    "    while True:\n",
    "        api_endpoint = f\"{base_api_endpoint}?$select={columns}&$limit={limit}&$offset={offset}\"\n",
    "        response = requests.get(api_endpoint, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            page_data = response.json()\n",
    "    \n",
    "            if not page_data:\n",
    "                break\n",
    "            all_data.extend(page_data)\n",
    "            offset += limit\n",
    "            print(f\"Retrieved {offset} records so far...\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve data:\", response.status_code)\n",
    "            break\n",
    "    \n",
    "    geodf_tree_data = pd.DataFrame(all_data)\n",
    "    geodf_tree_data.dropna(subset=['tree_id', 'latitude', 'longitude', 'status', 'health', 'spc_common', 'zipcode'], inplace=True)\n",
    "    return geodf_tree_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "747ff49f-a18b-4fc0-8da6-6834a10d11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data():\n",
    "    zillow_rent_data = pd.read_csv('data/drive-download-20231124T164450Z-001/zillow_rent_data.csv')\n",
    "\n",
    "    \n",
    "    nyc_zip_codes = zillow_rent_data[zillow_rent_data['RegionName'].astype(str).str.match(r'^1\\d{4}$')]\n",
    "\n",
    "    columns_to_delete = [\"RegionID\", \"SizeRank\", \"RegionType\", \"StateName\", \"State\", \"City\", \"Metro\", \"CountyName\"]\n",
    "    df = nyc_zip_codes.drop(columns=columns_to_delete)\n",
    "\n",
    "    df_long = df.melt(id_vars=[\"RegionName\"], \n",
    "                    var_name=\"Date\", \n",
    "                    value_name=\"RentPrice\")\n",
    "\n",
    "\n",
    "    df_long['Date'] = pd.to_datetime(df_long['Date'])\n",
    "\n",
    "    df_long = df_long.sort_values(by=['RegionName', 'Date'])\n",
    "\n",
    "    \n",
    "    df_long['RentPrice'] = df_long.groupby('RegionName')['RentPrice'].ffill()\n",
    "    df_long['RentPrice'] = df_long.groupby('RegionName')['RentPrice'].bfill()\n",
    "    df_long['RentPrice'] = df_long.groupby('RegionName')['RentPrice'].ffill().bfill().interpolate(method='linear')\n",
    "\n",
    "    zillow_data = pd.DataFrame(df_long)\n",
    "    return zillow_data\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345ebc2c-14f1-490c-9857-11f1e332e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    geodf_311_data = download_and_clean_311_data()\n",
    "    geodf_tree_data = download_and_clean_tree_data()\n",
    "    df_zillow_data = load_and_clean_zillow_data()\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60da57f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:   0%|          | 0/10 [00:00<?, ?it/s]0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Fetching Data:  10%|█         | 1/10 [00:48<07:14, 48.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:49<01:18, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n",
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:50<00:42,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  50%|█████     | 5/10 [00:50<00:23,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:50<00:12,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [00:51<00:06,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [00:51<00:03,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [05:00<01:18, 78.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [05:00<00:00, 30.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [01:06<03:38, 27.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 2200000...\n",
      "Retrieved records up to offset 2200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [01:07<01:47, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 2200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [01:07<00:57,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 2200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  50%|█████     | 5/10 [01:11<00:37,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 2200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [01:57<01:22, 20.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 2200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [01:58<00:41, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 2200000...\n",
      "Retrieved records up to offset 2200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [01:58<00:19,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 2200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [01:59<00:00, 11.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 2200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  10%|█         | 1/10 [00:33<05:04, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 4200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [00:34<01:54, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 4200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:35<00:58,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 4200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:36<00:31,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 4200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:40<00:12,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 4200000...\n",
      "Retrieved records up to offset 4200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [00:41<00:03,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 4200000...\n",
      "Retrieved records up to offset 4200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [00:42<00:00,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 4200000...\n",
      "Retrieved records up to offset 4200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching Data:  10%|█         | 1/10 [00:32<04:51, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [00:33<01:52, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:33<00:54,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:34<00:29,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  50%|█████     | 5/10 [00:39<00:24,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [00:40<00:07,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n",
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [00:40<00:03,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [00:40<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [00:43<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 6200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  10%|█         | 1/10 [00:33<05:01, 33.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 8200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [00:33<01:51, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 8200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:39<01:12, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 8200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:40<00:38,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 8200000...\n",
      "Retrieved records up to offset 8200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:40<00:12,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 8200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [00:40<00:07,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 8200000...\n",
      "Retrieved records up to offset 8200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [00:41<00:01,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 8200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [00:41<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 8200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  10%|█         | 1/10 [00:36<05:29, 36.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [00:36<02:01, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:37<00:59,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:38<00:34,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  50%|█████     | 5/10 [00:40<00:20,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:41<00:12,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [00:41<00:06,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [00:42<00:03,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [00:46<00:02,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [00:47<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 10200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  10%|█         | 1/10 [00:34<05:11, 34.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 12200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [00:34<01:55, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 12200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:35<00:55,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 12200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:35<00:30,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 12200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:38<00:11,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 12200000...\n",
      "Retrieved records up to offset 12200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [00:38<00:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 12200000...\n",
      "Retrieved records up to offset 12200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [00:39<00:01,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 12200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [00:41<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 12200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [00:45<02:30, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 14200000...\n",
      "Retrieved records up to offset 14200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:46<00:38,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 14200000...\n",
      "Retrieved records up to offset 14200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  50%|█████     | 5/10 [00:46<00:20,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 14200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:51<00:17,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 14200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [02:22<01:37, 32.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 14200000...\n",
      "Retrieved records up to offset 14200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [02:22<00:15, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 14200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [02:23<00:00, 14.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 14200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [01:12<03:59, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 16200000...\n",
      "Retrieved records up to offset 16200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [01:12<01:54, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 16200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [01:13<01:00, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 16200000...\n",
      "Retrieved records up to offset 16200000...\n",
      "Retrieved records up to offset 16200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [01:13<00:11,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 16200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [02:43<00:47, 23.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 16200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [02:44<00:18, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 16200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [02:47<00:00, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 16200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching Data:  20%|██        | 2/10 [00:43<02:23, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 18200000...\n",
      "Retrieved records up to offset 18200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:43<01:08,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 18200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  50%|█████     | 5/10 [00:44<00:20,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 18200000...\n",
      "Retrieved records up to offset 18200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:46<00:13,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 18200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [00:46<00:07,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 18200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [01:21<00:25, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 18200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [01:21<00:00,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 18200000...\n",
      "Retrieved records up to offset 18200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [00:35<01:56, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 20200000...\n",
      "Retrieved records up to offset 20200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:35<00:57,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 20200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:37<00:33,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 20200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:39<00:11,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 20200000...\n",
      "Retrieved records up to offset 20200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [00:39<00:06,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 20200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [00:41<00:03,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 20200000...\n",
      "Retrieved records up to offset 20200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [00:42<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 20200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  10%|█         | 1/10 [00:39<05:54, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  20%|██        | 2/10 [00:39<02:11, 16.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  30%|███       | 3/10 [00:40<01:04,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  40%|████      | 4/10 [00:45<00:44,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  60%|██████    | 6/10 [00:45<00:13,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n",
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  70%|███████   | 7/10 [00:45<00:06,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  80%|████████  | 8/10 [00:46<00:03,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [00:46<00:01,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [00:47<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 22200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data:  90%|█████████ | 9/10 [00:43<00:02,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 24200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data: 100%|██████████| 10/10 [00:48<00:00,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved records up to offset 24200000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 50000 records so far...\n",
      "Retrieved 100000 records so far...\n",
      "Retrieved 150000 records so far...\n",
      "Retrieved 200000 records so far...\n",
      "Retrieved 250000 records so far...\n",
      "Retrieved 300000 records so far...\n",
      "Retrieved 350000 records so far...\n",
      "Retrieved 400000 records so far...\n",
      "Retrieved 450000 records so far...\n",
      "Retrieved 500000 records so far...\n",
      "Retrieved 550000 records so far...\n",
      "Retrieved 600000 records so far...\n",
      "Retrieved 650000 records so far...\n",
      "Retrieved 700000 records so far...\n"
     ]
    }
   ],
   "source": [
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ad8bbc-bf91-457e-97db-a945fabeee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 262 entries, 0 to 262\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   ZIPCODE     262 non-null    object  \n",
      " 1   BLDGZIP     262 non-null    object  \n",
      " 2   PO_NAME     262 non-null    object  \n",
      " 3   POPULATION  262 non-null    float64 \n",
      " 4   AREA        262 non-null    float64 \n",
      " 5   STATE       262 non-null    object  \n",
      " 6   COUNTY      262 non-null    object  \n",
      " 7   ST_FIPS     262 non-null    object  \n",
      " 8   CTY_FIPS    262 non-null    object  \n",
      " 9   URL         262 non-null    object  \n",
      " 10  SHAPE_AREA  262 non-null    float64 \n",
      " 11  SHAPE_LEN   262 non-null    float64 \n",
      " 12  geometry    262 non-null    geometry\n",
      "dtypes: float64(4), geometry(1), object(8)\n",
      "memory usage: 28.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about each dataframe\n",
    "geodf_zipcode_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec68f4be-f365-46c1-91a1-ab75deb75ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>BLDGZIP</th>\n",
       "      <th>PO_NAME</th>\n",
       "      <th>POPULATION</th>\n",
       "      <th>AREA</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>ST_FIPS</th>\n",
       "      <th>CTY_FIPS</th>\n",
       "      <th>URL</th>\n",
       "      <th>SHAPE_AREA</th>\n",
       "      <th>SHAPE_LEN</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>0</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>18681.0</td>\n",
       "      <td>2.269930e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Queens</td>\n",
       "      <td>36</td>\n",
       "      <td>081</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((-73.806 40.683, -73.806 40.683, -73....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>62426.0</td>\n",
       "      <td>2.963100e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>36</td>\n",
       "      <td>047</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((-73.937 40.680, -73.935 40.680, -73....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>83866.0</td>\n",
       "      <td>4.197210e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>36</td>\n",
       "      <td>047</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((-73.903 40.671, -73.902 40.668, -73....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>56527.0</td>\n",
       "      <td>2.369863e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>36</td>\n",
       "      <td>047</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((-73.958 40.671, -73.956 40.670, -73....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>72280.0</td>\n",
       "      <td>3.686880e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>36</td>\n",
       "      <td>047</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((-73.972 40.651, -73.972 40.650, -73....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ZIPCODE BLDGZIP   PO_NAME  POPULATION          AREA STATE  COUNTY ST_FIPS  \\\n",
       "0   11436       0   Jamaica     18681.0  2.269930e+07    NY  Queens      36   \n",
       "1   11213       0  Brooklyn     62426.0  2.963100e+07    NY   Kings      36   \n",
       "2   11212       0  Brooklyn     83866.0  4.197210e+07    NY   Kings      36   \n",
       "3   11225       0  Brooklyn     56527.0  2.369863e+07    NY   Kings      36   \n",
       "4   11218       0  Brooklyn     72280.0  3.686880e+07    NY   Kings      36   \n",
       "\n",
       "  CTY_FIPS                   URL  SHAPE_AREA  SHAPE_LEN  \\\n",
       "0      081  http://www.usps.com/         0.0        0.0   \n",
       "1      047  http://www.usps.com/         0.0        0.0   \n",
       "2      047  http://www.usps.com/         0.0        0.0   \n",
       "3      047  http://www.usps.com/         0.0        0.0   \n",
       "4      047  http://www.usps.com/         0.0        0.0   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-73.806 40.683, -73.806 40.683, -73....  \n",
       "1  POLYGON ((-73.937 40.680, -73.935 40.680, -73....  \n",
       "2  POLYGON ((-73.903 40.671, -73.902 40.668, -73....  \n",
       "3  POLYGON ((-73.958 40.671, -73.956 40.670, -73....  \n",
       "4  POLYGON ((-73.972 40.651, -73.972 40.650, -73....  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 5 entries about each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a803b68-2f07-44b8-8b24-d4f16c9e03fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24336519 entries, 0 to 24336518\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   unique_key      object\n",
      " 1   created_date    object\n",
      " 2   closed_date     object\n",
      " 3   complaint_type  object\n",
      " 4   incident_zip    object\n",
      " 5   latitude        object\n",
      " 6   longitude       object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14705df9-ea77-4d57-ac8e-1845f80a216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>created_date</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>incident_zip</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58337824</td>\n",
       "      <td>2023-07-28T12:00:00.000</td>\n",
       "      <td>2023-08-01T12:00:00.000</td>\n",
       "      <td>Derelict Vehicles</td>\n",
       "      <td>11234</td>\n",
       "      <td>40.635140353456194</td>\n",
       "      <td>-73.92947230891184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58341879</td>\n",
       "      <td>2023-07-28T12:00:00.000</td>\n",
       "      <td>2023-07-29T12:00:00.000</td>\n",
       "      <td>Derelict Vehicles</td>\n",
       "      <td>10027</td>\n",
       "      <td>40.8111347014436</td>\n",
       "      <td>-73.94554917541718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58339183</td>\n",
       "      <td>2023-07-28T12:00:00.000</td>\n",
       "      <td>2023-07-28T12:00:00.000</td>\n",
       "      <td>Derelict Vehicles</td>\n",
       "      <td>10452</td>\n",
       "      <td>40.84613889606357</td>\n",
       "      <td>-73.91895485824278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58337816</td>\n",
       "      <td>2023-07-28T12:00:00.000</td>\n",
       "      <td>2023-07-28T12:00:00.000</td>\n",
       "      <td>Derelict Vehicles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58337822</td>\n",
       "      <td>2023-07-28T12:00:00.000</td>\n",
       "      <td>2023-07-29T12:00:00.000</td>\n",
       "      <td>Derelict Vehicles</td>\n",
       "      <td>11418</td>\n",
       "      <td>40.69691921183129</td>\n",
       "      <td>-73.84283078706525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_key             created_date              closed_date  \\\n",
       "0   58337824  2023-07-28T12:00:00.000  2023-08-01T12:00:00.000   \n",
       "1   58341879  2023-07-28T12:00:00.000  2023-07-29T12:00:00.000   \n",
       "2   58339183  2023-07-28T12:00:00.000  2023-07-28T12:00:00.000   \n",
       "3   58337816  2023-07-28T12:00:00.000  2023-07-28T12:00:00.000   \n",
       "4   58337822  2023-07-28T12:00:00.000  2023-07-29T12:00:00.000   \n",
       "\n",
       "      complaint_type incident_zip            latitude           longitude  \n",
       "0  Derelict Vehicles        11234  40.635140353456194  -73.92947230891184  \n",
       "1  Derelict Vehicles        10027    40.8111347014436  -73.94554917541718  \n",
       "2  Derelict Vehicles        10452   40.84613889606357  -73.91895485824278  \n",
       "3  Derelict Vehicles          NaN                 NaN                 NaN  \n",
       "4  Derelict Vehicles        11418   40.69691921183129  -73.84283078706525  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6006cd2-3a00-4660-8d2a-a660b9bfd91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 652167 entries, 0 to 683787\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   tree_id     652167 non-null  object\n",
      " 1   latitude    652167 non-null  object\n",
      " 2   longitude   652167 non-null  object\n",
      " 3   status      652167 non-null  object\n",
      " 4   health      652167 non-null  object\n",
      " 5   spc_common  652167 non-null  object\n",
      " 6   zipcode     652167 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 39.8+ MB\n"
     ]
    }
   ],
   "source": [
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f880ef-c5fc-4159-8174-21ccd44f492d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>status</th>\n",
       "      <th>health</th>\n",
       "      <th>spc_common</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180683</td>\n",
       "      <td>40.72309177</td>\n",
       "      <td>-73.84421522</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Fair</td>\n",
       "      <td>red maple</td>\n",
       "      <td>11375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200540</td>\n",
       "      <td>40.79411067</td>\n",
       "      <td>-73.81867946</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Fair</td>\n",
       "      <td>pin oak</td>\n",
       "      <td>11357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204026</td>\n",
       "      <td>40.71758074</td>\n",
       "      <td>-73.9366077</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>11211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204337</td>\n",
       "      <td>40.71353749</td>\n",
       "      <td>-73.93445616</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>11211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189565</td>\n",
       "      <td>40.66677776</td>\n",
       "      <td>-73.97597938</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>American linden</td>\n",
       "      <td>11215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tree_id     latitude     longitude status health       spc_common zipcode\n",
       "0  180683  40.72309177  -73.84421522  Alive   Fair        red maple   11375\n",
       "1  200540  40.79411067  -73.81867946  Alive   Fair          pin oak   11357\n",
       "2  204026  40.71758074   -73.9366077  Alive   Good      honeylocust   11211\n",
       "3  204337  40.71353749  -73.93445616  Alive   Good      honeylocust   11211\n",
       "4  189565  40.66677776  -73.97597938  Alive   Good  American linden   11215"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59724f74-5f1e-435c-b843-f381a875dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58380 entries, 311 to 58142\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   RegionName  58380 non-null  int64         \n",
      " 1   Date        58380 non-null  datetime64[ns]\n",
      " 2   RentPrice   58380 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "df_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ae5d9-9768-4590-a2f2-dd63b07dd712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionName</th>\n",
       "      <th>Date</th>\n",
       "      <th>RentPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>10001</td>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>3807.657462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>10001</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>3851.098684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>10001</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>3844.716691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>10001</td>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>3906.689196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>10001</td>\n",
       "      <td>2015-05-31</td>\n",
       "      <td>3960.689870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RegionName       Date    RentPrice\n",
       "311        10001 2015-01-31  3807.657462\n",
       "867        10001 2015-02-28  3851.098684\n",
       "1423       10001 2015-03-31  3844.716691\n",
       "1979       10001 2015-04-30  3906.689196\n",
       "2535       10001 2015-05-31  3960.689870"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685942c-26dc-40db-84c2-a71aa3340806",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349fbdd-67d0-40a4-97a0-d9b8c8ec8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_new_postgis_database(username, db_name):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ed80d-7b60-484f-a123-23b673b0f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_new_postgis_database(DB_USER, DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a251c-f337-4b24-bb41-96ee4621a9bd",
   "metadata": {},
   "source": [
    "### Creating Tables\n",
    "\n",
    "\n",
    "These are just a couple of options to creating your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d72390-3c2d-4856-82c0-3284e8ccb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac07405-dc2b-47af-9dad-6a9b94d2b34c",
   "metadata": {},
   "source": [
    "#### Option 1: SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d0cc6-74b3-4d35-a454-57f647c9f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the SQL statements to create your 4 tables\n",
    "ZIPCODE_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "NYC_311_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "NYC_TREE_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "ZILLOW_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d86f6-ff6e-4bb8-8fa2-df0d4282e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DB_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(ZIPCODE_SCHEMA)\n",
    "    f.write(NYC_311_SCHEMA)\n",
    "    f.write(NYC_TREE_SCHEMA)\n",
    "    f.write(ZILLOW_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eacd37-4fd7-4768-b689-88b07d5c234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using SQL (as opposed to SQLAlchemy), execute the schema files to create tables\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a2d89-b276-4d44-a0ef-3631eb686e84",
   "metadata": {},
   "source": [
    "#### Option 2: SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c42e1-4ad4-4c43-a2ba-1dbcac1de557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class Tree(Base):\n",
    "    __tablename__ = \"trees\"\n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a9c3d-e6d6-4e01-8247-e9d465381ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88a50c-9528-4a5c-9a52-b96781ee8985",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "These are just a couple of options to write data to your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66af67-afb8-4f0d-bb57-552972f8e4b8",
   "metadata": {},
   "source": [
    "#### Option 1: SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e37800-cd95-44b5-9c21-eb7ac2b2e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(tablename_to_dataframe):\n",
    "    # write INSERT statements or use pandas/geopandas to write SQL\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f042f5-8270-477d-929a-872f7d9a0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename_to_dataframe = {\n",
    "    \"zipcodes\": geodf_zipcode_data,\n",
    "    \"complaints\": geodf_311_data,\n",
    "    \"trees\": geodf_tree_data,\n",
    "    \"rents\": df_zillow_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d052c50-1e43-4356-bcac-4f5abc7e714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(tablename_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4708cb4-d034-43b6-955b-a21d0eab74d4",
   "metadata": {},
   "source": [
    "#### Option 2: SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ddbad-3b11-47a2-9245-935f482fa7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = db.orm.sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b21c6-59d9-4bae-8c33-ab4b49ff2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in geodf_tree_data.iterrows():\n",
    "    tree = Tree(...)\n",
    "    session.add(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4bfb9-4fbc-45fe-8a98-a760a22234f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63b553-0c64-4da8-9fc7-41555d89d853",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7e12b-e251-4f08-8dc5-601db30c2089",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce8548-4aba-4bf9-992c-dedd0f249db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605e6f3-ec42-4a8b-833c-5138c14b678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = QUERY_DIR / \"FILL_ME_IN\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "FILL_ME_IN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce044adf-ecdf-4237-9b20-b7cdaaab0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_1))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b2c3d-8961-4c7e-8eb1-fc973d0ab9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75223ce5-6ab5-4613-b6af-fa8e33bcc7d5",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fcfed-ddbb-4908-a60e-ed7cbc6d5b00",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e2cde-e43b-407b-ab93-ff85a2dba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80755f-d1e1-4e53-8ef8-f5295c59a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query your database for the data needed.\n",
    "    # You can put the data queried into a pandas/geopandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2632a-b516-4a6e-8b67-97116ab6fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
